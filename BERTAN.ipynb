{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4c7430",
   "metadata": {},
   "source": [
    "### Sinan Parmar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7c7cdb",
   "metadata": {},
   "source": [
    "In this notebook we will train a BERT model from scratch on Turkish Language and implement a fine-tuning task on sentiment analysis with this pretrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fea7ff",
   "metadata": {},
   "source": [
    "# 1 - BERTAN: Pretraining of BERT on Turkish Corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c89b8f",
   "metadata": {},
   "source": [
    "## 1.1 - Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3199a9d",
   "metadata": {},
   "source": [
    "### 1.1.1 - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1075fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are using the OSCAR dataset which is a combination of web crawls. \n",
    "#This dataset is availabe for donwload using the dataset package\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be34afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset oscar (/Users/mrp3anut/.cache/huggingface/datasets/oscar/unshuffled_deduplicated_tr/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bba1431495d494b9d3ced114f2edfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Download the dataset.\n",
    "\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3d6ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text'],\n",
       "        num_rows: 11596446\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bab968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70e0f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Son y覺llarda g繹r羹len ay tutulmalar覺na g繹re daha etkili olaca覺 s繹ylenen Kanl覺 veya K覺rm覺z覺 Ay Tutulmas覺na saatler kald覺. Bu akam (27 Temmuz 2018) ger癟ekleecek olan bu tutulmay覺 羹lkemizin her yerinde'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55895f99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac35ead809164b4aa3f0b2173c14cedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11596446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing tqdm for visuals.\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "\n",
    "#Filling text_data to a list by using new line seprator to seperate list elements.\n",
    "n_samples = 0\n",
    "\n",
    "for sample in tqdm(dataset['train']):\n",
    "    sample = sample['text'].replace('\\n', '')\n",
    "    text_data.append(sample)\n",
    "    if n_samples==len(dataset[\"train\"])//100:\n",
    "        break\n",
    "    else:\n",
    "        n_samples+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a61a184",
   "metadata": {},
   "source": [
    "### 1.1.2 - Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96370c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide data into train, test and validation splits.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_data_train, text_data_testval = train_test_split(text_data, test_size=0.2, random_state=42, shuffle=True)\n",
    "text_data_test, text_data_val = train_test_split(text_data_testval, test_size=0.1, random_state=10, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b3bc4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 92772\n",
      "Test size: 20873\n",
      "Val size: 2320\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: {}\".format(len(text_data_train)))\n",
    "print(\"Test size: {}\".format(len(text_data_test)))\n",
    "print(\"Val size: {}\".format(len(text_data_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48a35b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write data to text files to save for later use and tokenizer training.\n",
    "\n",
    "with open(f'./data/text_micro_train.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data_train))\n",
    "    \n",
    "with open(f'./data/text_micro_test.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data_test))\n",
    "    \n",
    "with open(f'./data/text_micro_val.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f637ffdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train tokenizer for vocab-token mapping.\n",
    "\n",
    "import tokenizers\n",
    " \n",
    "bwpt = tokenizers.BertWordPieceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79722217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import glob to get the filepaths of the text files.\n",
    "from glob import glob\n",
    "\n",
    "filepaths = glob(\"./data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd61382d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Train a tokenizer to tokenize and words/word-pairs.\n",
    "\n",
    "bwpt.train(\n",
    "    files=filepaths,\n",
    "    vocab_size=50000,\n",
    "    min_frequency=3,\n",
    "    limit_alphabet=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b265dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BERTANmicro/vocab.txt']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bwpt.save_model(\"BERTANmicro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8e96c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import BertTokenizerFast and LineByLineTextDataset to read the data by using our trained tokenizer.\n",
    "from transformers import BertTokenizerFast, LineByLineTextDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af271c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"./BERTANmicro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa73de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training and validation datasets using the trained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60acea93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mrp3anut/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/data/datasets/language_modeling.py:121: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the  Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "val_dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = './data/text_micro_val.txt',\n",
    "    block_size = 512  # maximum sequence length\n",
    ")\n",
    "\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = './data/text_micro_train.txt',\n",
    "    block_size = 512  # maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dda1307",
   "metadata": {},
   "source": [
    "## 1.2 - Model config and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea261ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a BERT config model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5e0e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of parameters:  8245200\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=50000,\n",
    "    hidden_size=128, \n",
    "    num_hidden_layers=2, \n",
    "    num_attention_heads=4,\n",
    "    max_position_embeddings=512\n",
    ")\n",
    " \n",
    "model = BertForMaskedLM(config)\n",
    "print('No of parameters: ', model.num_parameters())\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "096bf9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "#Enter training arguments, trainer and define callback for early stopping.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./BERTANmicro/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    logging_steps =10000,\n",
    "    save_steps=10000,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    prediction_loss_only=True,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end = True,\n",
    "    use_mps_device = True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3469ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b12f1c",
   "metadata": {},
   "source": [
    "## 1.3 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbdfa698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, BertForMaskedLM, EarlyStoppingCallback\n",
    "import torch\n",
    "\n",
    "#Empty GPU memmory.\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load BERTAN\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"./BERTAN/checkpoint-370000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cf33982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating features from dataset file at ./data/text_micro_test.txt\n"
     ]
    }
   ],
   "source": [
    "test_dataset = LineByLineTextDataset(\n",
    "    tokenizer = tokenizer,\n",
    "    file_path = './data/text_micro_test.txt',\n",
    "    block_size = 512  # maximum sequence length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60f41595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "#Enter evaluation arguments, trainer and define callback for early stopping.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./BERTANmicro/',\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=8,\n",
    "    use_mps_device = True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96982de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate\n",
    "model_eval = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982576d5",
   "metadata": {},
   "source": [
    "# 2 - Fine-tuning BERTAN for Turkish Sentiment Classification on Product Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b275c8ed",
   "metadata": {},
   "source": [
    "## 2.1 - Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f124c",
   "metadata": {},
   "source": [
    "### 2.1.1 - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159c26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48\n",
      "Found cached dataset csv (/Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1290d177b64a56b7b3b01ffeaf5c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Again we use a dataset from HugginFace's datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "tsentiment = load_dataset(\"winvoker/turkish-sentiment-analysis-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ad22cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '羹r羹n羹 hepsiburadadan alal覺 3 hafta oldu. orjinal ve eksiksiz ekilde geldi. arj konusunda 1 g羹n羹 rahat 癟覺kar覺yor oyun oynamama ramen. teslimat s羹recide h覺zl覺 ger癟ekleti. en uygun fiyata iphone kalitesi ka癟覺rmay覺n..',\n",
       " 'label': 'Positive',\n",
       " 'dataset': 'urun_yorumlari'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsentiment[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252916e",
   "metadata": {},
   "source": [
    "### 2.1.2 - Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5acddb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-05a0471dcb2f7a30.arrow and /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-f1f9b112779e29e9.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d9c24b5afa7fbe0a.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-b51d0f7654fe28b2.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-013d84435940c6f1.arrow\n"
     ]
    }
   ],
   "source": [
    "#split our data into train,validation and test splits\n",
    "\n",
    "train_val_dataset = tsentiment[\"train\"].train_test_split(test_size=0.01, seed=54)\n",
    "\n",
    "\n",
    "train_dataset = train_val_dataset[\"train\"].shuffle(seed=2)\n",
    "val_dataset = train_val_dataset[\"test\"].shuffle(seed=2)\n",
    "test_dataset = tsentiment[\"test\"].shuffle(seed=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4536f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn labels to integers\n",
    "\n",
    "train_label = []\n",
    "for label in train_dataset[\"label\"]:\n",
    "    if label==\"Negative\":\n",
    "        train_label.append(0)\n",
    "        \n",
    "    if label==\"Notr\":\n",
    "        train_label.append(1)\n",
    "        \n",
    "    if label==\"Positive\":\n",
    "        train_label.append(2)\n",
    "\n",
    "test_label = []\n",
    "for label in test_dataset[\"label\"]:\n",
    "    if label==\"Negative\":\n",
    "        test_label.append(0)\n",
    "        \n",
    "    if label==\"Notr\":\n",
    "        test_label.append(1)\n",
    "        \n",
    "    if label==\"Positive\":\n",
    "        test_label.append(2)\n",
    "\n",
    "val_label = []\n",
    "for label in val_dataset[\"label\"]:\n",
    "    if label==\"Negative\":\n",
    "        val_label.append(0)\n",
    "        \n",
    "    if label==\"Notr\":\n",
    "        val_label.append(1)\n",
    "        \n",
    "    if label==\"Positive\":\n",
    "        val_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9e3fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-1c9b7640a12338ff.arrow\n",
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2ab5c4a195f1699d.arrow\n",
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9ef29fb605cb25eb.arrow\n"
     ]
    }
   ],
   "source": [
    "#Replace text labels with text and remove some columns\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"label\",\"dataset\"])\n",
    "val_dataset = val_dataset.remove_columns([\"label\",\"dataset\"])\n",
    "test_dataset = test_dataset.remove_columns([\"label\",\"dataset\"])\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"label\",train_label)\n",
    "val_dataset = val_dataset.add_column(\"label\",val_label)\n",
    "test_dataset = test_dataset.add_column(\"label\",test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da6a7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pre-trained BERTAN tokenizer.\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./BERTAN/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0394e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create as preprocess function to process our data.\n",
    "#As our data is small we won't be using LineByLineText so we need to tokenize beforehand.\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e251c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-312c9f1e7770cfc2.arrow\n",
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-3cb885b9f1c2beb7.arrow\n",
      "Loading cached processed dataset at /Users/mrp3anut/.cache/huggingface/datasets/winvoker___csv/winvoker--turkish-sentiment-analysis-dataset-d9120c0cfbe0af48/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5c2f01bad9d9733f.arrow\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the datasets\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "#A little bit more cleaning...\n",
    "tokenized_train = tokenized_train.remove_columns([\"text\"])\n",
    "tokenized_test = tokenized_test.remove_columns([\"text\"])\n",
    "tokenized_val = tokenized_val.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757ea67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2,\n",
       " 1583,\n",
       " 9168,\n",
       " 32456,\n",
       " 1585,\n",
       " 57,\n",
       " 1845,\n",
       " 10831,\n",
       " 1566,\n",
       " 3119,\n",
       " 13065,\n",
       " 2018,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 9918,\n",
       " 6954,\n",
       " 8267,\n",
       " 6792,\n",
       " 1911,\n",
       " 1710,\n",
       " 8548,\n",
       " 31453,\n",
       " 46127,\n",
       " 1821,\n",
       " 1870,\n",
       " 20423,\n",
       " 16847,\n",
       " 2982,\n",
       " 2335,\n",
       " 31,\n",
       " 2561,\n",
       " 1655,\n",
       " 6853,\n",
       " 4653,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 2335,\n",
       " 5521,\n",
       " 2345,\n",
       " 17867,\n",
       " 2569,\n",
       " 1785,\n",
       " 1974,\n",
       " 18,\n",
       " 10755,\n",
       " 1566,\n",
       " 6794,\n",
       " 7860,\n",
       " 27250,\n",
       " 21792,\n",
       " 12234,\n",
       " 31,\n",
       " 13,\n",
       " 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273f3b0f",
   "metadata": {},
   "source": [
    "## 2.2 - Model config and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7329082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the data loader for the model and define compute metrics for training.\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "698737e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./BERTAN/checkpoint-370000/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50000\n",
      "}\n",
      "\n",
      "loading weights file ./BERTAN/checkpoint-370000/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ./BERTAN/checkpoint-370000/ were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./BERTAN/checkpoint-370000/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, BertForSequenceClassification, EarlyStoppingCallback\n",
    "import torch\n",
    "\n",
    "#Empty GPU memmory.\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load BERTAN\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./BERTAN/checkpoint-370000/\", num_labels=3)\n",
    "\n",
    "#Define training arguments and callbacks\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tsentiment_BERTAN/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=500,\n",
    "    save_total_limit=5,\n",
    "    prediction_loss_only=False,\n",
    "    metric_for_best_model = 'accuracy',\n",
    "    load_best_model_at_end = True,\n",
    "    use_mps_device=True\n",
    ")\n",
    "\n",
    "#Train\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440131f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 436272\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 109068\n",
      "  Number of trainable parameters = 81916419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='109068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    48/109068 00:25 < 16:49:14, 1.80 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Train\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/trainer.py:2508\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2508\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2511\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/trainer.py:2540\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2539\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1552\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1546\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1548\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1552\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1566\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1007\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1015\u001b[0m     embedding_output,\n\u001b[1;32m   1016\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/phys496/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:237\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    236\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 237\u001b[0m     embeddings \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    238\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    239\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119c68e",
   "metadata": {},
   "source": [
    "## 2.3 - Evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5ccc18",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 24\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#Enter evaluation arguments, trainer and define callback for early stopping.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     14\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./tsentiment_BERTAN/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     use_mps_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     22\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     23\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m---> 24\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[43mtokenized_test\u001b[49m,\n\u001b[1;32m     25\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     26\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     27\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_test' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "#Empty GPU memmory.\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#Load BERTAN\n",
    "model = BertForSequenceClassification.from_pretrained(\"./tsentiment_BERTAN/checkpoint-11000/\", num_labels=3)\n",
    "    \n",
    "#Enter evaluation arguments, trainer and define callback for early stopping.\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tsentiment_BERTAN/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    use_mps_device=True\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d8967a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48965\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09338823705911636,\n",
       " 'eval_accuracy': 0.9683651587868886,\n",
       " 'eval_runtime': 859.4299,\n",
       " 'eval_samples_per_second': 56.974,\n",
       " 'eval_steps_per_second': 7.122}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "db12baa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 48965\n",
      "  Batch size = 8\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe7c5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = evaluate.load(\"f1\")\n",
    "f1 = f1_metric.compute(predictions=pred[0].argmax(-1), references=tokenized_test[\"label\"], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "810dd477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.9680109183650836}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c51a454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = evaluate.load(\"roc_auc\",\"multilabel\")\n",
    "\n",
    "\n",
    "ref = np.zeros((len(tokenized_test[\"label\"]), max(tokenized_test[\"label\"]) + 1))\n",
    "ref[np.arange(len(tokenized_test[\"label\"])), tokenized_test[\"label\"]] = 1\n",
    "\n",
    "roc_auc = roc_auc.compute(prediction_scores=pred[0], references=ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "12be4245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.9922663277468269}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0c675",
   "metadata": {},
   "source": [
    "# 3 - Hands-on with both models: MLM with BERTAN and Sentiment Classification with duygusalBERTAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8e809",
   "metadata": {},
   "source": [
    "# 3.1 Masked Language Modeling with BERTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e7ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast, pipeline\n",
    "\n",
    "#Load model, tokenizer and setup mask filling pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"./BERTAN/checkpoint-370000/\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./BERTAN/\")\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051658d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19067342579364777,\n",
       "  'token': 25731,\n",
       "  'token_str': 'yand覺',\n",
       "  'sequence': 'bugun eve giderken yere dustum ve can覺m yand覺.'},\n",
       " {'score': 0.1750452071428299,\n",
       "  'token': 6748,\n",
       "  'token_str': 'cekti',\n",
       "  'sequence': 'bugun eve giderken yere dustum ve can覺m cekti.'},\n",
       " {'score': 0.09005220234394073,\n",
       "  'token': 26688,\n",
       "  'token_str': 'yan覺yor',\n",
       "  'sequence': 'bugun eve giderken yere dustum ve can覺m yan覺yor.'},\n",
       " {'score': 0.07144200801849365,\n",
       "  'token': 42287,\n",
       "  'token_str': 's覺k覺ld覺',\n",
       "  'sequence': 'bugun eve giderken yere dustum ve can覺m s覺k覺ld覺.'},\n",
       " {'score': 0.05144298076629639,\n",
       "  'token': 21414,\n",
       "  'token_str': 'sagolsun',\n",
       "  'sequence': 'bugun eve giderken yere dustum ve can覺m sagolsun.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Bug羹n eve giderken yere d羹t羹m ve can覺m [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c7d13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9608170390129089,\n",
       "  'token': 42646,\n",
       "  'token_str': 'camiinde',\n",
       "  'sequence': 'aksam namaz覺 gec saatte nafi baba camiinde k覺l覺n覺d覺.'},\n",
       " {'score': 0.01153489574790001,\n",
       "  'token': 7348,\n",
       "  'token_str': 'evinde',\n",
       "  'sequence': 'aksam namaz覺 gec saatte nafi baba evinde k覺l覺n覺d覺.'},\n",
       " {'score': 0.003500595223158598,\n",
       "  'token': 2001,\n",
       "  'token_str': 'taraf覺ndan',\n",
       "  'sequence': 'aksam namaz覺 gec saatte nafi baba taraf覺ndan k覺l覺n覺d覺.'},\n",
       " {'score': 0.002813145285472274,\n",
       "  'token': 11090,\n",
       "  'token_str': 'koyunde',\n",
       "  'sequence': 'aksam namaz覺 gec saatte nafi baba koyunde k覺l覺n覺d覺.'},\n",
       " {'score': 0.0022763388697057962,\n",
       "  'token': 8669,\n",
       "  'token_str': 'camii',\n",
       "  'sequence': 'aksam namaz覺 gec saatte nafi baba camii k覺l覺n覺d覺.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Akam namaz覺 ge癟 saatte Nafi Baba [MASK] k覺l覺n覺d覺.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dff4363d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.22464631497859955,\n",
       "  'token': 6148,\n",
       "  'token_str': 'matematik',\n",
       "  'sequence': 'okuldaki en zor ders matematik dersi'},\n",
       " {'score': 0.0655607357621193,\n",
       "  'token': 4126,\n",
       "  'token_str': 'ingilizce',\n",
       "  'sequence': 'okuldaki en zor ders ingilizce dersi'},\n",
       " {'score': 0.03280796855688095,\n",
       "  'token': 24860,\n",
       "  'token_str': 'geometri',\n",
       "  'sequence': 'okuldaki en zor ders geometri dersi'},\n",
       " {'score': 0.023688795045018196,\n",
       "  'token': 42131,\n",
       "  'token_str': 'zili',\n",
       "  'sequence': 'okuldaki en zor ders zili dersi'},\n",
       " {'score': 0.02036508359014988,\n",
       "  'token': 4340,\n",
       "  'token_str': 'fizik',\n",
       "  'sequence': 'okuldaki en zor ders fizik dersi'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"Okuldaki en zor ders [MASK] dersi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca47f694",
   "metadata": {},
   "source": [
    "# 3.2 Sentiment Classification with duygusalBERTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6f27ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, pipeline\n",
    "\n",
    "#Load model, tokenizer and setup mask filling pipeline\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"./tsentiment_BERTAN/checkpoint-13000/\", num_labels=3)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"./BERTAN/\")\n",
    "\n",
    "text_classify = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfbe596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.992276132106781}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classify(\"Ne kadar k繹t羹 bir telefon, Steve Jobs gelse y羹z羹n羹ze t羹k羹r羹d羹.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea3e7500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9725152254104614}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classify(\"Gen癟letim resmen bu kadar m覺 farkeder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e79a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEUTRAL', 'score': 0.9999432563781738}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classify(\"Arjantin milli futbol tak覺m覺n覺n kaptan覺 Lionel Messi'dir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd60d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEUTRAL', 'score': 0.9990272521972656}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_classify(\"襤nekler sulak ortamda otlanmay覺 sever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472f193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
